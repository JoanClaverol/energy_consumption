---
title: "The forecaster's toolbox"
output:
  html_document:
    df_print: paged
---

## Simple forecasting methods

```{r}
require(pacman)
p_load(fpp2, tidyverse, lubridate)
data <- read_rds("../Datasets/CleanWeatherData.rds")
data <- data %>% filter(year(date)!=2011)
```

Creation of a time series per day:

```{r}
ts <- ts(data$ActiveEnergy, start = 2007, frequency = 365.25)
h <- 90
```

### Average method

```{r}
plot(meanf(ts, h)) # h is the forecast horizon, 90 days
```

### Naïve method 

```{r}
# set all forecasts to be the value of the last observation. This method works remarkably well for many economic and financial time series.
plot(naive(ts, h))
plot(rwf(ts, h))
# Because a naïve forecast is optimal when data follow a random walk (see Section 8.1), these are also called random walk forecasts.
```

### Seasonal naïve method

```{r}
# A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year
plot(snaive(ts, h))
# very good predictions 
```

### Drift method

```{r}
# A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. 
plot(rwf(ts, h, drift=TRUE))
```

### Ploting different forecasts

```{r}
# Plot some seasonal forecasts 
autoplot(ts) +
  autolayer(meanf(ts, h),
            series="Mean", PI=FALSE) +
  autolayer(naive(ts, h),
            series="Naïve", PI=FALSE) +
  autolayer(snaive(ts, h),
            series="Seasonal naïve", PI=FALSE) +
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
# Plot some non-seasonal forecasts 
autoplot(goog200) +
  autolayer(meanf(goog200, h=40),
            series="Mean", PI=FALSE) +
  autolayer(rwf(goog200, h=40),
            series="Naïve", PI=FALSE) +
  autolayer(rwf(goog200, drift=TRUE, h=40),
            series="Drift", PI=FALSE) +
  ggtitle("Google stock (daily ending 6 Dec 2013)") +
  xlab("Day") + ylab("Closing Price (US$)") +
  guides(colour=guide_legend(title="Forecast"))
```

## Transformation and adjustments

1. calendar adjustments
2. population adjustments
3. Inflation adjustments
4. Mathematical transformations

### Calendar adjustments

```{r}
# Some of the variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before fitting a forecasting model. The monthdays() function will compute the number of days in each month or quarter.

data %>% 
  mutate(year = year(date), month = month(date, label = T, locale = "us")) %>% group_by(year, month) %>% 
  summarise(mean = mean(ActiveEnergy)) -> data2

ts_month <- ts(data2[,3], start = 2007, frequency = 12)

ts_month_norm <- cbind(ts_month, 
                       DailyAverage = ts_month/monthdays(ts_month))
autoplot(ts_month_norm, facet=TRUE) +
  xlab("Years") + ylab("Watts/h") +
  ggtitle("Active energy")
```

### Mathematical tranformations

```{r}
# Mathematical transformations
lambda <- BoxCox.lambda(ts_month)
autoplot(BoxCox(ts_month, lambda))
```

## Residual diagnosis

Proporties of the residuals:

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Any forecasting method that does not satisfy these properties can be improved. Also has the following properties:

3. The residuals have constant variance.
4. The residuals are normally distributed.

### Example non seasonal data

For stock market prices and indexes, the best forecasting method is often the naïve method. 

```{r}
t <- ts_month
autoplot(t) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day")
```

Let's check at the errors using the naïve method, :

```{r}
autoplot(snaive(t, h = 18))

res <- residuals(snaive(t, h = 18))
autoplot(res) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day")
```

Understanding the errors distribution through histograms:

```{r}
gghistogram(res, bins = 30) + ggtitle("Histogram of residuals")
```

Looking for errors in the distribution of the data, we can see there are a lot of errors and they have correlatiob between them:

```{r}
ggAcf(res) + ggtitle("ACF of residuals")
```

### Using Portmanteau tests for autocorrelation

```{r}
checkresiduals(naive(t))
```

## Training and tests sets

The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The test set should ideally be at least as large as the maximum forecast horizon required. The following points should be noted.

- A model which fits the training data well will not necessarily forecast well.
- A perfect fit can always be obtained by using a model with enough parameters.
- Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data.

Functions to subset time series:

- window:

```{r}
window(t, start = 2010)
```

- subset:

```{r}
subset(t, start = length(t)-4*5) # select 1/5 of the data
```

Or using subset function to extract values for a specific month, for the whole years:

```{r}
subset(t, month = 1)
```

- head and tail()

```{r}
tail(t, 4*5)
```

### Forecast errors

We can measure forecast accuracy by summarising the forecast errors in different ways:

#### Scale-dependent errors

The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:

* Mean absolute error: MAE

* Root mean squared error: RMSE

When comparing forecast methods applied to a single time series, or to several time series with the same units, the MAE is popular as it is easy to both understand and compute. A forecast method that minimises the MAE will lead to forecasts of the median, while minimising the RMSE will lead to forecasts of the mean. Consequently, the RMSE is also widely used, despite being more difficult to interpret.

#### Percentage errors

Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. The most commonly used measure is:

* Mean absolute percentage error: MAPE

Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero.2 For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on either the Fahrenheit or Celsius scales, because temperature has an arbitrary zero point.

They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called “symmetric” MAPE (sMAPE) proposed by Armstrong (1978, p. 348), which was used in the M3 forecasting competition. It is defined by:

* sMAPE

Hyndman & Koehler (2006) recommend that the sMAPE not be used. It is included here only because it is widely used, although we will not use it in this book.

#### Scaled errors

The mean absolute scaled error is simply MASE.

```{r}
t2 <- window(t,start=2007,end=c(2009,10))
fit1 <- meanf(t2,h=12)
fit2 <- rwf(t2,h=12)
fit3 <- snaive(t2,h=12)
autoplot(window(t2, start=2007)) +
  autolayer(fit1, series="Mean", PI=FALSE) +
  autolayer(fit2, series="Naïve", PI=FALSE) +
  autolayer(fit3, series="Seasonal naïve", PI=FALSE) +
  xlab("Time") + ylab("Watts/h") +
  ggtitle("Avg. energy consumped by day") +
  guides(colour=guide_legend(title="Forecast"))
```

Let's see the metric results:

```{r}
t3 <- window(t, start=c(2009,10))
accuracy(fit1, t3) # meanf
accuracy(fit2, t3) # rwf, drift method
accuracy(fit3, t3) # snaïve
```

#### Non seasonal example

```{r}
googfc1 <- meanf(goog200, h=40)
googfc2 <- rwf(goog200, h=40)
googfc3 <- rwf(goog200, drift=TRUE, h=40)
autoplot(subset(goog, end = 240)) +
  autolayer(googfc1, PI=FALSE, series="Mean") +
  autolayer(googfc2, PI=FALSE, series="Naïve") +
  autolayer(googfc3, PI=FALSE, series="Drift") +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))
```

```{r}
googtest <- window(goog, start=201, end=240)
accuracy(googfc1, googtest)
accuracy(googfc2, googtest) 
accuracy(googfc3, googtest)
```

## Time series cross-validation

```{r}
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))
#> [1] 6.233 # RMSE with cross validation
sqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))
#> [1] 6.169 # RMSE without cross validation
```

As expected, the RMSE from the residuals is smaller, as the corresponding “forecasts” are based on a model fitted to the entire data set, rather than being true forecasts.

A good way to choose the best forecasting model is to find the model with the smallest RMSE computed using time series cross-validation.

### Pipe operator

We can do the same thing than above but with pipes operators:

```{r}
goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e
e^2 %>% mean(na.rm=TRUE) %>% sqrt()
#> [1] 6.233
goog200 %>% rwf(drift=TRUE) %>% residuals() -> res
res^2 %>% mean(na.rm=TRUE) %>% sqrt()
#> [1] 6.169
```

### Example: using tsCV()

The code below evaluates the forecasting performance of 1- to 8-step-ahead naïve forecasts with tsCV(), using MSE as the forecast error measure. The plot shows that the forecast error increases as the forecast horizon increases, as we would expect.

```{r}
e <- tsCV(goog200, forecastfunction=naive, h=8)
# Compute the MSE values and remove missing values
mse <- colMeans(e^2, na.rm = T)
# Plot the MSE values against the forecast horizon
data.frame(h = 1:8, MSE = mse) %>%
  ggplot(aes(x = h, y = MSE)) + geom_point()
```

## Prediction intervals 

```{r}
autoplot(naive(goog200))
```

